# -*- coding: utf-8 -*-
"""Coding Assignment1 - Sentiment analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wJAEch-FdYkv855rGEKHY2Ri8msf-gYq

**Importing all required libraries.**
"""

import pandas as pd
import re
import numpy as np
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.corpus import  wordnet
from nltk.corpus import stopwords
from nltk.stem import  WordNetLemmatizer
from nltk.corpus import  wordnet
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
import gensim.downloader as api
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

"""**Initializing lemmatizer.**"""

lemmatizer = WordNetLemmatizer()

"""**Loading the dataset.**"""

df = pd.read_csv('/content/dataset.csv')

"""**Data Preprocessing.**"""

class TextPreprocessor:
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        self.pos_map = {
            'J': wordnet.ADJ,
            'N': wordnet.NOUN,
            'V': wordnet.VERB,
            'R': wordnet.ADV
        }

    def get_wordnet_pos(self, word):
        tag = nltk.pos_tag([word])[0][1][0].upper()
        return self.pos_map.get(tag, wordnet.NOUN)


    def lower_text(self, text):
        return text.lower()

    #Removing mentions and URLs
    def remove_mentions(self, text):
        return re.sub(r'(@\w+|http\S+|www\S+|https\S+|#)', '', text)

    #Removing hashtags, special characters and numbers
    def remove_hashtags(self, text):
        return re.sub(r'#\W+', '', text)

    #Removing extra spaces
    def remove_extra_spaces(self, text):
        return re.sub(r'\s+', ' ', text).strip()

    def lemmatize_text(self, text):
        words = word_tokenize(text)
        return ' '.join(self.lemmatizer.lemmatize(w, self.get_wordnet_pos(w)) for w in words)

    def preprocess(self, text):
        cleaned_text = self.lower_text(text)
        cleaned_text = self.remove_mentions(cleaned_text)
        cleaned_text = self.remove_hashtags(cleaned_text)
        cleaned_text = self.remove_extra_spaces(cleaned_text)
        return self.lemmatize_text(cleaned_text)

#Preprocessing
preprocessor = TextPreprocessor()
df['cleaned_text'] = df['text'].apply(preprocessor.preprocess)

"""**Comparing original and cleaned text for few rows.**"""

for original, cleaned in zip(df['text'].head(), df['cleaned_text'].head()):
    print(f"Original: {original}")
    print(f"Cleaned: {cleaned}")
    print("-" * 50)

"""**Splitting the dataset.**"""

# Features of cleaned text
X = df['cleaned_text']
# Labels
y = df['text_sentiment']

# Training: 80% and Testing: 20%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)

#Printing
print(f'Training set: {X_train.shape[0]} samples')
print(f'Testing set: {X_test.shape[0]} samples')

"""**Initializing stopwords from NLTK.**"""

stop_words = stopwords.words('english')

"""**Bag of Words Technique for Sentiment Analysis.**"""

bow_vectorizer = CountVectorizer(max_features=5000, stop_words=stop_words)
X_train_bow = bow_vectorizer.fit_transform(X_train)
X_test_bow = bow_vectorizer.transform(X_test)

#Selecting the top 100 features using the chi-square test.
chi2_selector = SelectKBest(chi2, k=100)

X_train_bow_chi2 = chi2_selector.fit_transform(X_train_bow, y_train)
X_test_bow_chi2 = chi2_selector.transform(X_test_bow)
feature_names_bow = bow_vectorizer.get_feature_names_out()

#Indices of the top 100 selected features from the chi-square selector.
top_100_indices = chi2_selector.get_support(indices=True)

#Top 100 features.
top_100_features_bow = [feature_names_bow[i] for i in top_100_indices]

# Printing the top 100 features
print("Top 100 features selected by chi-square test (BoW):")
for feature in top_100_features_bow:
    print(feature)

# SVM model using BoW with Chi-Square selected features.
model_svm_bow = SVC(kernel='linear')
model_svm_bow.fit(X_train_bow_chi2, y_train)

# Predicting and evaluating SVM model using BoW.
y_pred_svm_bow = model_svm_bow.predict(X_test_bow_chi2)
accuracy_svm_bow = accuracy_score(y_test, y_pred_svm_bow)
print(f'Accuracy using SVM with Bag of Words is: {accuracy_svm_bow:.4f}')

"""**TF_IDF Technique for Sentiment Analysis.**"""

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=stop_words)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

#Selecting the top 100 features using the chi-square test.
chi2_selector = SelectKBest(chi2, k=100)

# Applying the chi-square selection on TF-IDF features.
X_train_tfidf_chi2 = chi2_selector.fit_transform(X_train_tfidf, y_train)
X_test_tfidf_chi2 = chi2_selector.transform(X_test_tfidf)

# Getting feature names from the CountVectorizer.
feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()

#Indices of the top 100 selected features from the chi-square selector.
top_100_indices = chi2_selector.get_support(indices=True)

#Top 100 features.
top_100_features_tfidf = [feature_names_tfidf[i] for i in top_100_indices]

# Printing the top 100 features.
print("Top 100 features selected by chi-square test (TF-IDF):")
for feature in top_100_features_tfidf:
    print(feature)

# SVM model using TF-IDF with Chi-Square selected features.
model_svm_tfidf = SVC(kernel='linear')
model_svm_tfidf.fit(X_train_tfidf_chi2, y_train)

# Predicting and evaluating SVM model (TF-IDF).
y_pred_svm_tfidf = model_svm_tfidf.predict(X_test_tfidf_chi2)
accuracy_svm_tfidf = accuracy_score(y_test, y_pred_svm_tfidf)
print(f'Accuracy using SVM with TF-IDF: {accuracy_svm_tfidf:.4f}')

"""**Word2Vec Technique for Sentiment Analysis.**"""

# Loading pre-trained Word2Vec vectors from Google News pre-trained vectors 300 dimensions.
word2vec = api.load('word2vec-google-news-300')

# Function to convert a tweet to a Word2Vec vector by averaging word embeddings.
def word_to_vec(tweet, model, vector_size=300):
    words = tweet.split()
    word_vecs = [model[word] for word in words if word in model]
    if len(word_vecs) == 0:
        print(f"No word in the tweet is found in the model: {tweet}")
        return np.zeros(vector_size)
    return np.mean(word_vecs, axis=0)

# Applying the conversion to Word2Vec vectors for all tweets.
X = np.array([word_to_vec(tweet, word2vec) for tweet in df['cleaned_text']])
y = df['text_sentiment']

# Splitting the dataset into training: 80% and testing: 20% sets.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)

# Training a SVM model on the Word2Vec vectors.
model_w2v = SVC(kernel='linear')
model_w2v.fit(X_train, y_train)

# Predicting and evaluating the model.
y_pred_w2v = model_w2v.predict(X_test)
accuracy_w2v = accuracy_score(y_test, y_pred_w2v)
print(f'Accuracy using Word2Vec embeddings: {accuracy_w2v:.4f}')

"""**Glove for feature representation.**"""

!pip install torch==2.3.0 torchtext==0.18.0

import torch
import torchtext.vocab as vocab

# Loading pre-trained GloVe vectors from torchtext 300-dimensional vectors.
glove = vocab.GloVe(name='6B', dim=300)

# Function to convert a tweet to a GloVe vector by averaging word embeddings.
def tweet_to_glove_vec(tweet, model, vector_size=300):
    words = tweet.split()
    word_vecs = [model[word].numpy() for word in words if word in model.stoi]
    if len(word_vecs) == 0:
        print(f"No word in the tweet is found in the model: {tweet}")
        return np.zeros(vector_size)
    return np.mean(word_vecs, axis=0)

# Applying the conversion to GloVe vectors for all tweets.
X_glove = np.array([tweet_to_glove_vec(tweet, glove) for tweet in df['cleaned_text']])

# Splitting the dataset into training: 80% and testing: 20% sets.
X_train_glove, X_test_glove, y_train, y_test = train_test_split(X_glove, y, test_size=0.2, stratify=y, random_state=42)

# Training a Logistic Regression model on the GloVe vectors.
model_glove = SVC(kernel='linear')
model_glove.fit(X_train_glove, y_train)

# Predicting and evaluating the model.
y_pred_glove = model_glove.predict(X_test_glove)
accuracy_glove = accuracy_score(y_test, y_pred_glove)
print(f'Accuracy using GloVe embeddings: {accuracy_glove:.4f}')

"""**Sentence Vectors Technique for Sentiment Analysis.**"""

import tensorflow as tf
import tensorflow_hub as hub

class TextPreprocessor:
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        self.pos_map = {
            'J': wordnet.ADJ,
            'N': wordnet.NOUN,
            'V': wordnet.VERB,
            'R': wordnet.ADV
        }
        # Loading Universal Sentence Encoder
        self.use_model = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

    def get_wordnet_pos(self, word):
        tag = nltk.pos_tag([word])[0][1][0].upper()
        return self.pos_map.get(tag, wordnet.NOUN)

    def lower_text(self, text):
        return text.lower()

    #Removing mentions and URLs
    def remove_mentions(self, text):
        return re.sub(r'(@\w+|http\S+|www\S+|https\S+|#)', '', text)

    #Removing hashtags, special characters and numbers
    def remove_hashtags(self, text):
        return re.sub(r'#\W+', '', text)

    #Removing extra spaces
    def remove_extra_spaces(self, text):
        return re.sub(r'\s+', ' ', text).strip()

    def lemmatize_text(self, text):
        words = word_tokenize(text)
        return ' '.join(self.lemmatizer.lemmatize(w, self.get_wordnet_pos(w)) for w in words)

    def preprocess(self, text):
        cleaned_text = self.lower_text(text)
        cleaned_text = self.remove_mentions(cleaned_text)
        cleaned_text = self.remove_hashtags(cleaned_text)
        cleaned_text = self.remove_extra_spaces(cleaned_text)
        return self.lemmatize_text(cleaned_text)

    def get_sentence_vector(self, text):
        # Getting the USE embedding for the preprocessed text.
        embedding = self.use_model([text])
        # Converting to Python list.
        return embedding.numpy().tolist()[0]

preprocessor = TextPreprocessor()

# Applying preprocessing and getting sentence vectors.
df['cleaned_text'] = df['text'].apply(preprocessor.preprocess)
df['sentence_vector'] = df['cleaned_text'].apply(preprocessor.get_sentence_vector)

# Preparing the feature and target variables.
X = np.array(df['sentence_vector'].tolist())  # Convert sentence vectors to a numpy array
y = df['text_sentiment']  # Assuming sentiment labels are in 'text_sentiment'

# Splitting the data into training: 80% and test: 20% sets.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)

# Trainning an SVM classifier.
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train, y_train)

# Prediction on the test set.
y_pred = svm_model.predict(X_test)

# Calculating and printing the accuracy.
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy using SVM with Universal Sentence Encoder embeddings: {accuracy:.4f}")

"""**TF_IDF performed better than others with accuracy 0.7206.**"""